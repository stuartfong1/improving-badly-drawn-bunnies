{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-02-21T17:24:37.395000Z","iopub.status.busy":"2024-02-21T17:24:37.394150Z","iopub.status.idle":"2024-02-21T17:24:37.400579Z","shell.execute_reply":"2024-02-21T17:24:37.399457Z","shell.execute_reply.started":"2024-02-21T17:24:37.394969Z"},"trusted":true},"outputs":[],"source":["import numpy as np                                                                                                                                  \n","import matplotlib.pyplot as plt\n","import math\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","import torch.nn.functional as F\n","from torch.optim import Adam\n","from tqdm.auto import tqdm\n","import os\n","from datetime import datetime\n","from math import sqrt                   "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-21T17:24:37.402624Z","iopub.status.busy":"2024-02-21T17:24:37.402342Z","iopub.status.idle":"2024-02-21T17:24:37.415960Z","shell.execute_reply":"2024-02-21T17:24:37.415127Z","shell.execute_reply.started":"2024-02-21T17:24:37.402595Z"},"trusted":true},"outputs":[],"source":["# --------- VAE PARAMETERS --------------\n","\n","stroke_dim = 5\n","\n","# decoder https://arxiv.org/pdf/1704.03477.pdf\n","M = 20 # number of normal distributions for output \n","T = 0.01# temperature parameter\n","sqrtT = sqrt(T) # saves computation\n","\n","latent_dim = 128\n","dec_hidden_dim = 2048 # dimension of cell and hidden states\n","enc_hidden_dim = 512 # dimension of cell and hidden states\n","\n","# --------- TRAINING PARAMETERS ----------\n","\n","lr = 1e-4# Used to be 2e-3 but got NaN gradients\n","batch_size = 100 # batch_size >= 1, used to be 128\n","\n","n_epochs = 10 # Used to be 150\n","w_kl = 1.0 # weight for loss calculation, can be tuned if needed\n","anneal_loss = True # True if train using annealed kl loss, False otherwise\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","pretrained = False\n","\n","# Hyperparameters for annealing loss\n","n_min = 0.01  # Starting Value from paper\n","R = 0.9999  # R is a term close to but less than 1.\n","KL_min = 0.1 # Value from paper (needs to be between 0.1 and 0.5)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-21T17:24:37.417400Z","iopub.status.busy":"2024-02-21T17:24:37.417071Z","iopub.status.idle":"2024-02-21T17:24:37.425588Z","shell.execute_reply":"2024-02-21T17:24:37.424792Z","shell.execute_reply.started":"2024-02-21T17:24:37.417369Z"},"trusted":true},"outputs":[],"source":["def prune_data(data, num_std=2.5):\n","    \"\"\"\n","    Given a .npz file loaded in numpy arrays, prune the dataset based on its mean, std,\n","    and a user-specified number of std (default is 2.5).\n","\n","    Parameters:\n","        - data: a .npz file loaded in as numpy arrays.\n","        - num_std: optional int for specifying how many images should be pruned.\n","                    As num_std increases, less images are pruned, and vice versa.\n","                    This should be a value between 0 and 3.\n","\n","    Returns:\n","         - none\n","    \"\"\"\n","    # get initial # of images\n","    print(f\"Initial number of images: {np.size(data)}\")\n","\n","    # Get mean + std\n","    data_val = np.zeros(0)\n","    for i in data:\n","        data_val = np.append(data_val, [len(i)])\n","    mean = np.mean(data_val)\n","    std = np.std(data_val)\n","\n","    lower = math.floor(mean - std*num_std)  # the lower bound of points allowed\n","    upper = math.ceil(mean + std*num_std) # the upper bound of points allowed\n","\n","    i = 0\n","    while i < np.size(data):\n","        num_points = np.shape(data[i])[0] # gets number of points\n","        if num_points < lower or num_points > upper:\n","            data = np.delete(data, i)\n","        else:\n","            i += 1\n","\n","    # get final # of images\n","    print(f\"Number of images after pruning: {np.size(data)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-21T17:24:37.426919Z","iopub.status.busy":"2024-02-21T17:24:37.426624Z","iopub.status.idle":"2024-02-21T17:25:06.174263Z","shell.execute_reply":"2024-02-21T17:25:06.173266Z","shell.execute_reply.started":"2024-02-21T17:24:37.426893Z"},"trusted":true},"outputs":[],"source":["# --------- DATA LOADING -----------------\n","\n","#from python.pruning import prune_data \n","\n","# File paths for sketch data\n","#datapaths = [\"data/banana.npz\",\"data/apple.npz\"]\n","\n","datapaths = [\n","    \"data/cat.npz\",\n","    \"data/owl.npz\",\n","    \"data/crab.npz\",\n","    \"data/rabbit.npz\",\n","    \"data/skull.npz\",]\n","\n","Nclass = len(datapaths)\n","\n","datasets = [np.load(path, encoding='latin1', allow_pickle=True) for path in datapaths]\n","\n","for dataset in datasets:\n","  prune_data(dataset[\"train\"],1.8)\n","  prune_data(dataset[\"test\"])\n","\n","Nmax = max([\n","  max([len(i) for i in dataset[\"train\"]] + [len(i) for i in dataset[\"test\"]]) \n","  for dataset in datasets\n","  ])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-21T17:25:06.177653Z","iopub.status.busy":"2024-02-21T17:25:06.177327Z","iopub.status.idle":"2024-02-21T17:25:06.185720Z","shell.execute_reply":"2024-02-21T17:25:06.184794Z","shell.execute_reply.started":"2024-02-21T17:25:06.177628Z"},"trusted":true},"outputs":[],"source":["def encode_sketch(sketch,length,N,do_offset = True): \n","    ''' \n","    One-hot encode pen state by adding additional columns for pen up and end of stroke.\n","\n","    Parameters: \n","        sketch (ndarray): n*4 array with format (x,y,p1,class), representing sketch data\n","\n","    Returns: \n","        ndarray: n*5 array with format (x,y,p1,p2,p3, c1,...,c_N), where p2 = 1-p1 and p3 is 1 at \n","        end of the sketch, 0 otherwise.\n","    '''\n","    if do_offset:\n","        sketch[:,:2] *= np.random.uniform(0.9, 1.1, size=(Nmax, 2))\n","\n","\n","    shape = sketch.shape\n","    pen_up = (np.ones(shape[0]) - sketch[:,2]).reshape(shape[0],1)\n","    category = np.zeros((shape[0],N))\n","    \n","    category[:,int(sketch[0][-1])] = 1\n","    \n","    end_stroke = np.zeros((shape[0],1))\n","    end_stroke[length:] = 1 \n","    pen_up[length:] = 0\n","    sketch[:,2][length:] = 0\n","    sketch[-1][2] = 0\n","    \n","    return np.concatenate((sketch[:,:-1],pen_up,end_stroke,category),axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-21T17:25:06.187207Z","iopub.status.busy":"2024-02-21T17:25:06.186915Z","iopub.status.idle":"2024-02-21T17:25:06.196689Z","shell.execute_reply":"2024-02-21T17:25:06.195818Z","shell.execute_reply.started":"2024-02-21T17:25:06.187184Z"},"trusted":true},"outputs":[],"source":["def encode_dataset1(data,lengths,do_offset = True):\n","    \"\"\"\n","    Encode pen states by creating a new array of sketch data.\n","    \n","    Parameters:\n","        data (iterable): object containing data for each sketch\n","        \n","    Returns:\n","        ndarray: object array containing encoded data for each sketch\n","    \"\"\"\n","    # new_data = np.empty(data.size,dtype=object)\n","    new_data = np.empty((data.shape[0], data.shape[1], stroke_dim+Nclass), dtype=object)\n","\n","    for i, sketch in enumerate(data):\n","        new_data[i] = encode_sketch(sketch,lengths[i],Nclass,do_offset)\n","\n","    return new_data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-21T17:25:06.198043Z","iopub.status.busy":"2024-02-21T17:25:06.197728Z","iopub.status.idle":"2024-02-21T17:25:06.209928Z","shell.execute_reply":"2024-02-21T17:25:06.209004Z","shell.execute_reply.started":"2024-02-21T17:25:06.198021Z"},"trusted":true},"outputs":[],"source":["class SketchesDataset():\n","    def __init__(self, datasets, mode, transform=None):\n","        \"\"\"\n","        data_path: The path to the data\n","        mode: Either 'train' or 'test'\n","        \"\"\"\n","        self.transform = transform\n","        self.mode = mode\n","        self.data_set = []\n","        for i, dataset in enumerate(datasets):\n","            dataset = dataset[mode]\n","            dataset = normalize_data(dataset)\n","            for j, sketch in enumerate(dataset):\n","                sketch_class = np.full((len(sketch), 1), i)\n","                sketch = np.concatenate([sketch, sketch_class], 1)\n","                self.data_set.append(sketch)\n","\n","    def __len__(self):\n","        return len(self.data_set)\n","\n","    def __getitem__(self, idx):\n","        sketch = self.data_set[idx]\n","        if self.transform:\n","            sketch, length = self.transform(sketch)\n","        return sketch, length"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-21T17:25:06.211265Z","iopub.status.busy":"2024-02-21T17:25:06.211004Z","iopub.status.idle":"2024-02-21T17:25:06.220815Z","shell.execute_reply":"2024-02-21T17:25:06.220021Z","shell.execute_reply.started":"2024-02-21T17:25:06.211244Z"},"trusted":true},"outputs":[],"source":["def normalize_data(data):\n","\n","    total_length = 0\n","    \n","\n","    for element in data:\n","        total_length += (len(element))\n","\n","\n","    coordinate_list = np.empty((total_length,2))\n","\n","    i = 0\n","\n","    for element in data:\n","        coordinate_list[i:i+len(element),:] = element[:,0:2]\n","        i+=len(element)\n","\n","    data_std = np.std(coordinate_list)\n","\n","    for i, element in enumerate(data):\n","        data[i] = data[i].astype(np.float32)\n","        data[i][:,0:2] = element[:,0:2].astype(np.float32)/data_std\n","    \n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-21T17:25:06.222049Z","iopub.status.busy":"2024-02-21T17:25:06.221775Z","iopub.status.idle":"2024-02-21T17:25:06.230607Z","shell.execute_reply":"2024-02-21T17:25:06.229683Z","shell.execute_reply.started":"2024-02-21T17:25:06.222026Z"},"trusted":true},"outputs":[],"source":["def display_encoded_image(image):\n","    \"\"\"\n","    For some image tensor, draw the image using matplotlib.\n","\n","    Parameters:\n","        - image: some [n*5] tensor representing a sketch.\n","    Returns:\n","        - none\n","    \"\"\"\n","    #Xplot and Yplot are array of points that will be plotted\n","    Xplot = [0]\n","    Yplot = [0]\n","    #Keeps track of the current point that is being drawn\n","    xpos = 0\n","    ypos = 0\n","    #For loop to go through data and plot points\n","    i=0\n","    for i in range(len(image)):\n","        xpos += float(image[i,0])\n","        ypos += float(image[i,1])\n","        Xplot.append(-xpos)\n","        Yplot.append(-ypos)\n","        if image[i,3] == 0:\n","            plt.plot(Xplot, Yplot,color='black')\n","            Xplot.clear()\n","            Yplot.clear()\n","        # elif image[i, 4] == 1:\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-21T17:25:06.232364Z","iopub.status.busy":"2024-02-21T17:25:06.231837Z","iopub.status.idle":"2024-02-21T17:25:06.242390Z","shell.execute_reply":"2024-02-21T17:25:06.241629Z","shell.execute_reply.started":"2024-02-21T17:25:06.232333Z"},"trusted":true},"outputs":[],"source":["def kl_loss(sigma_hat, mu):\n","    # torch.sum is added to sum over all the dimensions of the vectors\n","    return (-0.5 / (latent_dim * batch_size)) * torch.sum(1 + sigma_hat - torch.square(mu) - torch.exp(sigma_hat))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-21T17:25:06.243776Z","iopub.status.busy":"2024-02-21T17:25:06.243466Z","iopub.status.idle":"2024-02-21T17:25:06.252058Z","shell.execute_reply":"2024-02-21T17:25:06.251137Z","shell.execute_reply.started":"2024-02-21T17:25:06.243725Z"},"trusted":true},"outputs":[],"source":["def make_image(image):\n","    length = len(image)\n","    new_image = np.zeros((Nmax, 4))\n","    new_image[:len(image), :] = image[:len(image), :] # copy over values\n","\n","    encoded_strokes = np.stack(encode_dataset1(np.array([new_image]),[length]), 1) # don't forget to stack input along dim = 1\n","    batch = torch.from_numpy(encoded_strokes.astype(float))\n","    return batch, torch.tensor(length)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-21T17:25:06.253527Z","iopub.status.busy":"2024-02-21T17:25:06.253278Z","iopub.status.idle":"2024-02-21T17:25:06.263171Z","shell.execute_reply":"2024-02-21T17:25:06.262279Z","shell.execute_reply.started":"2024-02-21T17:25:06.253505Z"},"trusted":true},"outputs":[],"source":["def make_batch(size=batch_size):\n","    \"\"\"\n","    Using the data created earlier in the code and a given batch size, randomly fetch\n","    that many images and return them + their lengths.\n","\n","    Parameters:\n","        - size: the size of the batch. Default is the variable batch_size declared\n","            at the start of the code.\n","\n","    Returns:\n","        - batch: a tensor of the batch of random images appended in the order they were fetched in.\n","        - lengths: the length of each image fetched, in the order they were fetched in.\n","    \"\"\"\n","\n","    batch_ids = np.random.choice(len(data), size)\n","    batch_images = [data[id] for id in batch_ids]\n","    lengths = [len(image) for image in batch_images]\n","    strokes = []\n","    for image in batch_images:\n","        new_image = np.zeros((Nmax, 3))\n","        new_image[:len(image), :] = image[:len(image), :] # copy over values\n","        strokes.append(new_image)\n","\n","    encoded_strokes = np.stack(encode_dataset1(np.array(strokes),lengths), 1) # don't forget to stack input along dim = 1\n","    batch = torch.from_numpy(encoded_strokes.astype(float))\n","    return batch, torch.tensor(lengths)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-21T17:25:06.264523Z","iopub.status.busy":"2024-02-21T17:25:06.264199Z","iopub.status.idle":"2024-02-21T17:25:06.299623Z","shell.execute_reply":"2024-02-21T17:25:06.298919Z","shell.execute_reply.started":"2024-02-21T17:25:06.264493Z"},"trusted":true},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self):\n","        super(Encoder, self).__init__()\n","        self.lstm = nn.LSTM(stroke_dim + Nclass, enc_hidden_dim, bidirectional=True)\n","\n","        self.fc_mu = nn.Linear(2*enc_hidden_dim, latent_dim)\n","        self.fc_sigma = nn.Linear(2*enc_hidden_dim, latent_dim)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Runs a batch of images through the encoder and returns its latent vector.\n","        Does not normalize values on its own.\n","\n","        Parameters:\n","         - x: Tensor of shape [max_strokes, batch_size, num_features]\n","            where max_strokes is the highest number of points possible for an image in the batch.\n","            x should be normalized.\n","        - batch_size: int representing the current batch size.\n","\n","        Returns:\n","        - mu: Tensor of shape [batch_size, 2*hidden dim] representing the mean of the distribution of values\n","        - sigma: Tensor of shape [batch_size, 2*hidden dim] representing the log of the distribution of values\n","        \"\"\"\n","\n","        # Get the hidden states\n","        hidden, cell = torch.zeros(2, (batch_size), enc_hidden_dim,device=device), torch.zeros(2, (batch_size), enc_hidden_dim,device=device)\n","\n","        _, (hidden, cell) = self.lstm(x.float(), (hidden, cell))\n","        hidden_forward_dir, hidden_backward_dir = torch.split(hidden, 1, 0)\n","        hidden_concatenated = torch.cat([hidden_forward_dir.squeeze(0), hidden_backward_dir.squeeze(0)], 1)\n","\n","        mu = self.fc_mu(hidden_concatenated)\n","        sigma = self.fc_sigma(hidden_concatenated)\n","        return mu, sigma\n","\n","encoder = Encoder()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-21T17:25:06.303492Z","iopub.status.busy":"2024-02-21T17:25:06.303238Z","iopub.status.idle":"2024-02-21T17:25:06.311136Z","shell.execute_reply":"2024-02-21T17:25:06.310191Z","shell.execute_reply.started":"2024-02-21T17:25:06.303471Z"},"trusted":true},"outputs":[],"source":["def gaussian_mixture_model(mixture_weights, mean_x, mean_y, std_x, std_y, corr_xy):\n","    \"\"\"\n","    Input:\n","        mixture_weights: Mixture weights (probability of a point being in distribution i)\n","        mean_x: The x-values of the means of each distribution\n","        mean_y: The y-values of the means of each distribution\n","        std_x: The standard deviations of the x-values\n","        std_y: The standard deviations of the y-values\n","        corr_xy: The correlation coefficients of the x and y values\n","        \n","    Return: \n","        The sampled x and y offsets\n","    \"\"\"\n","    # Choose which distribution to sample from\n","    mixture_weights = torch.reshape(mixture_weights,(batch_size,M)).contiguous() \n","     \n","    # Index for each batch\n","    i = torch.searchsorted(mixture_weights.cumsum(0), torch.rand(batch_size, 1)).squeeze()\n","    \n","    # Sample from bivariate normal distribution i\n","    rand_x, rand_y = torch.randn(batch_size), torch.randn(batch_size)\n","    \n","    mean_x = torch.take(mean_x, i)\n","    mean_y = torch.take(mean_y, i)\n","    std_x = torch.take(std_x, i)\n","    std_y = torch.take(std_y, i)\n","    corr_xy = torch.take(corr_xy, i)\n","    \n","    # Alternatively torch.distributions.multivariate_normal.MultivariateNormal?\n","    offset_x = mean_x + std_x * rand_x\n","    offset_y = mean_y + std_y * (corr_xy * offset_x + torch.sqrt(1 - corr_xy ** 2) * rand_y)\n","    return offset_x.unsqueeze(0), offset_y.unsqueeze(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-21T17:25:06.312561Z","iopub.status.busy":"2024-02-21T17:25:06.312246Z","iopub.status.idle":"2024-02-21T17:25:06.323702Z","shell.execute_reply":"2024-02-21T17:25:06.322921Z","shell.execute_reply.started":"2024-02-21T17:25:06.312532Z"},"trusted":true},"outputs":[],"source":["def sample(mixture_weights, mean_x, mean_y, std_x, std_y, corr_xy, pen_state):\n","    offset_x, offset_y = gaussian_mixture_model(mixture_weights, mean_x, mean_y, std_x, std_y, corr_xy)\n","    \n","    pen_state = pen_state.view(batch_size,3)\n","    pen_state = torch.searchsorted(pen_state.cumsum(1), torch.rand(batch_size, 1)).squeeze()\n","    #assert(pen_state[0].item() <= 2 and pen_state[1].item() <= 2)\n","    next_point = torch.cat((offset_x, offset_y, torch.zeros(3, batch_size)))\n","    next_point = torch.cat((offset_x, offset_y,torch.eye(3)[pen_state].view(batch_size,3).transpose(0,1)))\n","    \n","    return next_point.transpose(0, 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-21T17:25:06.324924Z","iopub.status.busy":"2024-02-21T17:25:06.324637Z","iopub.status.idle":"2024-02-21T17:25:06.334633Z","shell.execute_reply":"2024-02-21T17:25:06.333769Z","shell.execute_reply.started":"2024-02-21T17:25:06.324902Z"},"trusted":true},"outputs":[],"source":["def distribution(decoder_output):\n","    \"\"\"\n","    Input: \n","        decoder_output (6M + 3): Decoder LSTM output\n","    Return:\n","        mixture_weights (M): Mixture weights (probability of a point being in distribution i)\n","        mean_x (M): The x-values of the means of each distribution\n","        mean_y (M): The y-values of the means of each distribution\n","        std_x (M): The standard deviations of the x-values\n","        std_y (M): The standard deviations of the y-values\n","        corr_xy (M): The correlation coefficients of the x and y values\n","        q (3): The predicted pen state (pen_down, pen_up, <EOS>)\n","    \"\"\"\n","    # Split the decoder output into \n","    # [pi, mean_x, mean_y, std_x, std_y, rho_xy] and [q1,q2,q3]\n","    parameters = torch.split(decoder_output, 6, 2)\n","\n","    # Chunk the parameters together, then stack them \n","    # so that each column defines a distribution\n","    mixture_parameters = torch.stack(parameters[:-1],1)\n","\n","    # Split mixture parameters into each parameter\n","    mixture_weights, mean_x, mean_y, std_x, std_y, corr_xy = torch.split(mixture_parameters, 1, 3)\n","\n","    # The 3 leftover parameters are for the pen state\n","    pen_state = parameters[-1]\n","\n","    mixture_weights = F.softmax(mixture_weights/T,dim=3)  # Each weight must be in [0, 1] and all must sum to 1\n","    std_x = torch.exp(std_x)*sqrtT  # Standard deviation must be positive\n","    std_y = torch.exp(std_y)*sqrtT  # Standard deviation must be positive\n","    corr_xy = F.tanh(corr_xy)  # Correlation coefficient must be in [-1, 1]\n","    pen_state = F.softmax(pen_state/T,dim=2)  # Each probability must be in [0, 1] and all must sum to 1\n","\n","    return mixture_weights, mean_x, mean_y, std_x, std_y, corr_xy, pen_state"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-21T17:25:06.336529Z","iopub.status.busy":"2024-02-21T17:25:06.336149Z","iopub.status.idle":"2024-02-21T17:25:06.348507Z","shell.execute_reply":"2024-02-21T17:25:06.347546Z","shell.execute_reply.started":"2024-02-21T17:25:06.336499Z"},"trusted":true},"outputs":[],"source":["input_dim = latent_dim + stroke_dim + Nclass # z | (x,y,p1,p2,p3) | (c1, c2, ... c_Nclass)\n","output_dim = 6*M + 3\n","\n","class Decoder(nn.Module):\n","    def __init__(self):\n","        super(Decoder, self).__init__()\n","\n","        # generates initial hidden and cell states from latent vector\n","        self.fc_in = nn.Linear(latent_dim,2*dec_hidden_dim)\n","\n","        # Fully connected layer for reducing dimensionality of hidden state before\n","        # being used for distribution parameters.\n","        self.fc_proj = nn.Linear(dec_hidden_dim,output_dim)\n","\n","        # Input has dimension latent_dim + 5 for latent vector and initial stroke\n","        self.lstm = nn.LSTM(input_dim,dec_hidden_dim)\n","\n","        self.hidden = torch.zeros(1,batch_size,dec_hidden_dim,device=device)\n","        self.cell = torch.zeros(1,batch_size,dec_hidden_dim,device=device)\n","    \n","    def forward(self, z, stroke, generate):\n","        \"\"\"\n","        Parameters:\n","            z - Tensor of size  (batch_size, latent_dim), with latent vector samples.\n","\n","            stroke - Tensor of size (batch_size, stroke dim); previous stroke\n","\n","\n","        Returns:\n","            Tensor of size (N_max, batch_size, stroke dim), as the next stroke\n","        \"\"\"\n","        \n","        if generate:\n","            x = torch.cat((stroke,z),dim = 1).view(1,batch_size,input_dim)\n","\n","            out, (self.hidden, self.cell) = self.lstm(x.float(),(self.hidden.contiguous(),self.cell.contiguous()))\n","\n","            # Sample from output distribution. If temperature parameter is small,\n","            # this becomes deterministic.\n","            with torch.device(device):\n","                params = distribution(self.fc_proj(out))\n","                next = sample(*params).to(device)\n","            return next, params\n","        \n","        else:\n","            self.hidden, self.cell = torch.split(\n","                F.tanh(self.fc_in(z).unsqueeze(0)),\n","                [dec_hidden_dim, dec_hidden_dim],\n","                dim = 2)\n","\n","            out, (self.hidden, self.cell) = self.lstm(stroke.float(),(self.hidden.contiguous(), self.cell.contiguous()))\n","\n","            params = distribution(self.fc_proj(out))\n","\n","        return params"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-21T17:25:06.350455Z","iopub.status.busy":"2024-02-21T17:25:06.349682Z","iopub.status.idle":"2024-02-21T17:25:06.361005Z","shell.execute_reply":"2024-02-21T17:25:06.360240Z","shell.execute_reply.started":"2024-02-21T17:25:06.350423Z"},"trusted":true},"outputs":[],"source":["def bivariate_normal_pdf(dx, dy, mu_x, mu_y, std_x, std_y, corr_xy):\n","    \"\"\"\n","    Return N(dx, dy | mu_x, mu_y, std_x, std_y, corr_xy)\n","    \"\"\"\n","    z_x = (dx - mu_x) / std_x\n","    z_y = (dy - mu_y) / std_y\n","    exponent = -(z_x ** 2 - 2 * corr_xy * z_x * z_y + z_y ** 2) / (2 * (1 - corr_xy ** 2))\n","    norm = 2 * np.pi * std_x * std_y * torch.sqrt(1-corr_xy ** 2)\n","    return torch.exp(exponent) / norm\n","\n","\n","def offset_reconstruction_loss(dx, dy, pi, mu_x, mu_y, std_x, std_y, corr_xy, mask):\n","    \"\"\"\n","    pi: The mixture probabilities\n","    mask: 1 if the point is not after the final stroke, 0 otherwise\n","\n","    Returns the reconstruction loss for the strokes, L_s\n","    \"\"\"\n","    pdf = bivariate_normal_pdf(dx, dy, mu_x, mu_y, std_x, std_y, corr_xy)\n","    \n","    return -torch.sum(mask * torch.log(1e-5 + torch.sum(pi*pdf,axis=0))) / (batch_size*Nmax) \n","\n","\n","def pen_reconstruction_loss(input_pen_state, output):\n","    \"\"\"\n","    Parameters:\n","\n","        N_max (int) - Maximum sketch sequence length\n","        \n","        input_pen_state (batch_size,3) - Pen state data for a stroke\n","\n","        output (batch_size, 3)- Generated pen state logit values.\n","\n","    Returns:\n","        Reconstruction loss for pen state.\n","    \"\"\"    \n","\n","    return -torch.sum(input_pen_state*torch.log(1e-5+output)) / (batch_size*Nmax)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-21T17:25:06.362842Z","iopub.status.busy":"2024-02-21T17:25:06.362313Z","iopub.status.idle":"2024-02-21T17:25:06.386671Z","shell.execute_reply":"2024-02-21T17:25:06.385954Z","shell.execute_reply.started":"2024-02-21T17:25:06.362812Z"},"trusted":true},"outputs":[],"source":["class VAE(nn.Module):\n","    def __init__(self):\n","        super(VAE, self).__init__()\n","        self.encoder = Encoder()\n","        self.decoder = Decoder()\n","        self.generate = False\n","    \n","    def run_decoder1(self,batch,lengths,z,classifier,compute_loss = True):\n","        '''\n","        Generates sketches conditioned only on latent vector and a classification vector\n","        \n","        Parameters:\n","            classifer (1,Nclass) - One-hot encoded (usually) vector to indicate sketch type.\n","            \n","            compute_loss - Set this to False if loss is not needed.\n","        '''\n","       \n","        # Obtain initial hidden and cell states by splitting result of fc_in along column axis\n","        (self.decoder.hidden, self.decoder.cell) = torch.split(\n","            F.tanh(self.decoder.fc_in(z).view(1,batch_size,2*dec_hidden_dim)),\n","            [dec_hidden_dim, dec_hidden_dim],\n","            dim = 2)\n","        \n","        # Data for all strokes in output sequence\n","        strokes = torch.zeros(Nmax + 1, batch_size, stroke_dim,device=device)\n","        strokes[0,:] = torch.tensor([0,0,1,0,0]).to(device)\n","        \n","        if compute_loss:\n","            mask, dx, dy, p = make_target(batch, lengths)\n","            l_p = torch.zeros(1).to(device)\n","            l_s = torch.zeros(1).to(device)\n","        else:\n","            l_p = 0\n","            l_s = 0\n","        \n","        # used when loop goes beyond input sketch length\n","        empty_stroke = torch.tensor([0,0,0,0,1]).to(torch.float32).to(device)\n","        \n","        # For each timestep, pass the batch of strokes through LSTM and compute\n","        # the output.  Output of the previous timestep is used as input.\n","        for i in range(1,Nmax):\n","            input = torch.cat((strokes[i-1],classifier),dim = 1)\n","            \n","            #params will be used for computing loss\n","            strokes[i], params = self.decoder(z,input,self.generate)\n","            \n","            #input_stroke = batch[i]\n","            \n","            if compute_loss:\n","                # calculate loss at each timestep. params[6] is pen_state, \n","                # input_stroke is the input data\n","                l_p += pen_reconstruction_loss(p[i],params[6])\n","                offset_params = [params[i].view(M,batch_size) for i in range(6)]\n","                l_s += offset_reconstruction_loss(\n","                    dx[i],\n","                    dy[i],\n","                    *offset_params,\n","                    mask[i]\n","                )\n","            #for strokes in generated sequence past sequence length, set to [0,0,0,0,1]\n","            stroke_mask = (strokes[i,:,4] == 1).to(device) # boolean mask set to true when p3 == 1\n","            strokes[i,stroke_mask.squeeze(),:] = empty_stroke\n","            \n","        return strokes[1:],l_s,l_p\n","\n","    def run_decoder2(self,batch,lengths,z):\n","\n","        empty = torch.zeros(stroke_dim + Nclass)\n","        empty[2] = 1\n","        start_stroke = torch.stack([empty] * batch_size).unsqueeze(0).to(device)\n","        strokes = torch.cat([start_stroke, batch[:-1]], 0)\n","        zs = torch.stack([z] * (Nmax))\n","        #IMPORTANT: Must always ensure that this is concatenated in the same order as the\n","        #generation mode in the decoder. \n","        strokes = torch.cat([strokes,zs], 2)\n","\n","        params = self.decoder(z, strokes,self.generate)\n","\n","        mask, dx, dy, p = make_target(batch, lengths)\n","        #compute loss\n","        offset_params = [params[i].view(Nmax,M,batch_size).transpose(0, 1) for i in range(6)]\n","        l_p = pen_reconstruction_loss(p, params[6]) \n","        l_s = offset_reconstruction_loss(dx, dy, *offset_params, mask[1:])\n","        \n","        output = torch.zeros(Nmax, batch_size, 5,device=device)\n","        with torch.device(device):\n","            empty_stroke = torch.tensor([0,0,0,0,1]).to(torch.float32).to(device)\n","            for i in range(Nmax):\n","                output[i] = sample(*[j[i] for j in params])\n","                stroke_mask = (i > lengths).squeeze()\n","                output[i,stroke_mask.squeeze(),:] = empty_stroke\n","                           \n","\n","        return output, l_s, l_p\n","    \n","    def forward(self, batch, lengths, anneal_loss=True, step=0): \n","        batch = batch.to(device)\n","        lengths = lengths.to(device) \n","        \n","        mean, logvar = self.encoder(batch)\n","        \n","        # sample latent vector from encoder output\n","        random_sample = torch.randn(batch_size, latent_dim, device = device)\n","        std = torch.exp(logvar/2) # logvar / 2 should be a float\n","        z = mean + std*random_sample\n","        \n","        \n","          \n","        if self.generate:\n","            output, l_s, l_p = self.run_decoder1(batch,lengths,z,batch[0,:,5:].clone())\n","        else:\n","            output, l_s, l_p = self.run_decoder2(batch,lengths,z)\n","\n","        l_r = l_p + l_s\n","        l_kl = kl_loss(mean, logvar)\n","        \n","        if anneal_loss:\n","            # Calculate n_step\n","            n_step = 1 - (1 - n_min) * R**step\n","\n","            # Calculate the total weighted loss\n","            loss = l_r + w_kl * n_step * max(l_kl, KL_min)\n","        else:\n","            loss = l_r + w_kl * l_kl\n","    \n","        return output, loss, l_kl, l_s, l_p"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-21T17:25:06.388624Z","iopub.status.busy":"2024-02-21T17:25:06.387936Z","iopub.status.idle":"2024-02-21T17:25:06.398828Z","shell.execute_reply":"2024-02-21T17:25:06.397999Z","shell.execute_reply.started":"2024-02-21T17:25:06.388593Z"},"trusted":true},"outputs":[],"source":["def make_target(batch, lengths):\n","    with torch.device(device):\n","        mask = torch.zeros((Nmax + 1, batch.size()[1]))\n","        for index, num_strokes in enumerate(lengths):\n","            mask[:num_strokes, index] = 1\n","\n","        dx = batch[:, :, 0]\n","        dy = batch[:, :, 1]\n","        # copy + append together pen state values\n","        p = torch.stack([batch.data[:, :, 2], batch.data[:, :, 3], batch.data[:, :, 4]], 2)\n","\n","        return mask, dx, dy, p"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-21T17:25:06.400165Z","iopub.status.busy":"2024-02-21T17:25:06.399871Z","iopub.status.idle":"2024-02-21T17:25:06.409435Z","shell.execute_reply":"2024-02-21T17:25:06.408517Z","shell.execute_reply.started":"2024-02-21T17:25:06.400142Z"},"trusted":true},"outputs":[],"source":["def save_model(dir):\n","    '''\n","    Saves model parameters to \"model/temp\".  An epoch is saved as t_{n}, where\n","    n enumerates the parameter files.  Note that these get VERY big.   \n","    '''\n","    \n","    try:\n","        os.makedirs(dir)\n","    except:\n","        pass\n","    \n","    filename = f\"{str(datetime.now())}.pt\"\n","    files = os.listdir(dir)\n","    \n","    while len(files) >= 3:\n","        os.remove(dir + files.pop(0))\n","        \n","    torch.save({\n","            \"model\": model.state_dict(),\n","            \"opt\": optimizer.state_dict()\n","        }, dir + filename)\n","    \n","def load_model(dir,file_idx):\n","    params = os.listdir(dir)\n","    loaded_state = torch.load(dir + f\"{params[0]}\", map_location=device)\n","    model.load_state_dict(loaded_state['model'])\n","    optimizer.load_state_dict(loaded_state['opt'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-21T17:25:06.411009Z","iopub.status.busy":"2024-02-21T17:25:06.410626Z","iopub.status.idle":"2024-02-21T17:25:06.421001Z","shell.execute_reply":"2024-02-21T17:25:06.420171Z","shell.execute_reply.started":"2024-02-21T17:25:06.410966Z"},"trusted":true},"outputs":[],"source":["#dir = \"model/temp/\"\n","#for file in os.listdir(dir):\n","#    os.remove(dir+file)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-21T17:25:06.422866Z","iopub.status.busy":"2024-02-21T17:25:06.422062Z","iopub.status.idle":"2024-02-21T17:25:18.292203Z","shell.execute_reply":"2024-02-21T17:25:18.291333Z","shell.execute_reply.started":"2024-02-21T17:25:06.422841Z"},"trusted":true},"outputs":[],"source":["train_dataset = SketchesDataset(\n","        datasets=datasets,\n","        mode='train',\n","        transform=make_image\n","    )\n","test_dataset = SketchesDataset(\n","        datasets=datasets,\n","        mode='test',\n","        transform=make_image\n","    )\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,drop_last=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True,drop_last=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-21T17:25:18.295803Z","iopub.status.busy":"2024-02-21T17:25:18.295159Z","iopub.status.idle":"2024-02-21T17:25:18.314657Z","shell.execute_reply":"2024-02-21T17:25:18.313712Z","shell.execute_reply.started":"2024-02-21T17:25:18.295775Z"},"trusted":true},"outputs":[],"source":["def train():\n","    print(\"Training loop running...\\n\")\n","\n","    for epoch in range(n_epochs):\n","        for step, (batch, lengths) in enumerate(tqdm(train_dataloader)):\n","            batch = batch.squeeze(2).transpose(0, 1)\n","\n","            optimizer.zero_grad()\n","\n","            output, loss, l_kl, l_s, l_p = model(batch, lengths, anneal_loss=True, step=epoch * len(train_dataloader) + step)\n","\n","            loss.backward()\n","\n","            grad_threshold = 1.0 # tunable parameter, prevents exploding gradient\n","            nn.utils.clip_grad_norm_(model.encoder.parameters(), grad_threshold)\n","            nn.utils.clip_grad_norm_(model.decoder.parameters(), grad_threshold)\n","\n","            # update encoder and decoder parameters using adam algorithm\n","            optimizer.step()\n","\n","            if step % 100 == 0:\n","                print(f\"Epoch: {epoch + 1}, Step: {step + 1}, Loss: {loss.item()}\")\n","                print(f\"l_kl: {l_kl.item():.4f} l_s: {l_s.item():.4f} l_p: {l_p.item():.4f}\") \n","                print(\"---------------------------------------------------------\\n\")\n","\n","            if step % len(train_dataloader)/(10*batch_size) == 0:\n","                  save_model(\"model/temp/\")\n","\n","            if step % 250 == 0:\n","                # draw image\n","                display_encoded_image(output[:, 0, :])\n","                display_encoded_image(batch[:, 0, :])\n","                \n","            torch.cuda.empty_cache()\n","                \n","def run_tests(count = 2**31 - 1):\n","    for step, (batch, lengths) in enumerate(tqdm(test_dataloader)):\n","            \n","            batch = batch.squeeze(2).transpose(0, 1)\n","            output, loss, l_kl, l_s, l_p = model(batch, lengths, anneal_loss=False)\n","            \n","            print(f\"Step: {step + 1}, Loss: {loss.item()}\")\n","            print(f\"l_kl: {l_kl.item():.4f} l_s: {l_s.item():.4f} l_p: {l_p.item():.4f}\") \n","            print(\"---------------------------------------------------------\\n\")\n","            # draw image\n","            display_encoded_image(output[:, 0, :])\n","            display_encoded_image(batch[:, 0, :])\n","            \n","            if (step > count):\n","                return"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-21T17:25:18.316769Z","iopub.status.busy":"2024-02-21T17:25:18.315882Z","iopub.status.idle":"2024-02-21T17:25:18.545125Z","shell.execute_reply":"2024-02-21T17:25:18.543408Z","shell.execute_reply.started":"2024-02-21T17:25:18.316711Z"},"trusted":true},"outputs":[],"source":["model = VAE().to(device)\n","optimizer = Adam(model.parameters(), lr = lr)    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-21T17:25:18.547155Z","iopub.status.busy":"2024-02-21T17:25:18.546808Z","iopub.status.idle":"2024-02-21T17:25:18.551408Z","shell.execute_reply":"2024-02-21T17:25:18.550521Z","shell.execute_reply.started":"2024-02-21T17:25:18.547123Z"},"trusted":true},"outputs":[],"source":["#load_model(\"model/temp/\",1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-21T17:25:18.553014Z","iopub.status.busy":"2024-02-21T17:25:18.552724Z"},"trusted":true},"outputs":[],"source":["model.generate = False\n","T = 0.001\n","train()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["save_model(\"model/final/\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.generate = True\n","T = 0.0001\n","run_tests(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def latent_lerp(model, S1, S2, nstep):\n","    mean, logvar = model.encoder(S1)\n","    z1 = mean + torch.exp(logvar/2)*torch.randn(batch_size, latent_dim, device = device)\n","    mean, logvar = model.encoder(S2)\n","    z2 = mean + torch.exp(logvar/2)*torch.randn(batch_size, latent_dim, device = device)\n","    \n","    c1 = S1[0,:,5:].clone()\n","    c2 = S2[0,:,5:].clone()\n","    \n","    step_size = 1/(nstep - 1)\n","    \n","    z_interp = [torch.lerp(z1, z2, step_size*i) for i in range(nstep)]\n","    c_interp = [torch.lerp(c1, c2, step_size*i) for i in range(nstep)]\n","    \n","    return [model.run_decoder1(None, None, z, c, compute_loss = False)[0] for z, c in zip(z_interp, c_interp)]\n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# INTERPOLATION EXAMPLE\n","def get_sketch(dataloader):\n","    return next(iter(dataloader))[0].squeeze(2).transpose(0, 1).to(device)\n","\n","S1 = get_sketch(test_dataloader)\n","S2 = get_sketch(test_dataloader)\n","display_encoded_image(S1[:, 0, :])\n","display_encoded_image(S2[:, 0, :])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["nstep = 10\n","\n","T =0.05\n","\n","interp = latent_lerp(model,S1,S2,nstep)\n","\n","for i in range(nstep):\n","    display_encoded_image(interp[i][:, 0, :])\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":22085,"sourceId":28618,"sourceType":"datasetVersion"}],"dockerImageVersionId":30646,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
