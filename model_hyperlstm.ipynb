{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%pip install labml_helpers\n","%pip install labml_nn"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import numpy as np                                                                                                                                  \n","import matplotlib.pyplot as plt\n","import math\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","import torch.nn.functional as F\n","from torch.optim import Adam\n","from tqdm.auto import tqdm\n","import os\n","from datetime import datetime\n","from math import sqrt \n","from typing import Optional, Tuple\n","from labml_helpers.module import Module\n","from labml_nn.lstm import LSTMCell          "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# --------- VAE PARAMETERS --------------\n","\n","stroke_dim = 5\n","\n","# decoder https://arxiv.org/pdf/1704.03477.pdf\n","M = 20 # number of normal distributions for output \n","T = 1# temperature parameter\n","# saves computation\n","\n","latent_dim = 128\n","feature_dim = 32\n","dec_hyper_dim = 64\n","dec_hidden_dim = 2048 # dimension of cell and hidden states\n","enc_hidden_dim = 512 # dimension of cell and hidden states\n","\n","# --------- TRAINING PARAMETERS ----------\n","\n","lr = 7e-5# Used to be 2e-3 but got NaN gradients\n","batch_size =  2# batch_size >= 1, used to be 128\n","conditional = True\n","n_epochs = 10\n","w_kl = 0.99 # weight for loss calculation, can be tuned if needed\n","anneal_loss = True # True if train using annealed kl loss, False otherwise\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# Hyperparameters for annealing loss\n","n_min = 0.01  # Starting Value from paper\n","R = 0.9999  # R is a term close to but less than 1.\n","KL_min = 0.2 # Value from paper (needs to be between 0.1 and 0.5)\n","\n","# --------- DATA LOADING -----------------\n","\n","#from python.pruning import prune_data \n","\n","# File paths for sketch data\n","#datapaths = [\"data/banana.npz\",\"data/apple.npz\"]\n","\n","datapaths = [\n","  \"data/remote/apple.npz\",\n","  \"data/remote/flower.npz\",\n","  \"data/remote/cactus.npz\",\n","  \"data/remote/carrot.npz\"\n","    ]\n","\n","Nclass = len(datapaths) "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def prune_data(data, num_std=2.5):\n","    \"\"\"\n","    Given a .npz file loaded in numpy arrays, prune the dataset based on its mean, std,\n","    and a user-specified number of std (default is 2.5).\n","\n","    Parameters:\n","        - data: a .npz file loaded in as numpy arrays.\n","        - num_std: optional int for specifying how many images should be pruned.\n","                    As num_std increases, less images are pruned, and vice versa.\n","                    This should be a value between 0 and 3.\n","\n","    Returns:\n","         - none\n","    \"\"\"\n","    # get initial # of images\n","    print(f\"Initial number of images: {np.size(data)}\")\n","\n","    # Get mean + std\n","    data_val = np.zeros(0)\n","    for i in data:\n","        data_val = np.append(data_val, [len(i)])\n","    mean = np.mean(data_val)\n","    std = np.std(data_val)\n","\n","    lower = math.floor(mean - std*num_std)  # the lower bound of points allowed\n","    upper = math.ceil(mean + std*num_std) # the upper bound of points allowed\n","\n","    i = 0\n","    while i < np.size(data):\n","        num_points = np.shape(data[i])[0] # gets number of points\n","        if num_points < lower or num_points > upper:\n","            data = np.delete(data, i)\n","        else:\n","            i += 1\n","\n","    # get final # of images\n","    print(f\"Number of images after pruning: {np.size(data)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["datasets = [np.load(path, encoding='latin1', allow_pickle=True) for path in datapaths]\n","\n","for dataset in datasets:\n","  prune_data(dataset[\"train\"],1.5)\n","  prune_data(dataset[\"test\"],1.5)\n","\n","Nmax = max([\n","  max([len(i) for i in dataset[\"train\"]] + [len(i) for i in dataset[\"test\"]]) \n","  for dataset in datasets\n","  ])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def encode_sketch(sketch,length,N,do_offset = True): \n","    ''' \n","    One-hot encode pen state by adding additional columns for pen up and end of stroke.\n","\n","    Parameters: \n","        sketch (ndarray): n*4 array with format (x,y,p1,class), representing sketch data\n","\n","    Returns: \n","        ndarray: n*5 array with format (x,y,p1,p2,p3, c1,...,c_N), where p2 = 1-p1 and p3 is 1 at \n","        end of the sketch, 0 otherwise.\n","    '''\n","    if do_offset:\n","        sketch[:,:2] *= np.random.uniform(0.9, 1.1, size=(Nmax, 2))\n","\n","\n","    shape = sketch.shape\n","    pen_up = (np.ones(shape[0]) - sketch[:,2]).reshape(shape[0],1)\n","    \n","    end_stroke = np.zeros((shape[0],1))\n","    end_stroke[length:] = 1 \n","    pen_up[length:] = 0\n","    sketch[:,2][length:] = 0\n","    sketch[-1][2] = 0\n","    \n","    if N > 0:\n","        category = np.zeros((shape[0],N))\n","        category[:,int(sketch[0][-1])] = 1\n","        return np.concatenate((sketch[:,:-1],pen_up,end_stroke,category),axis=1)\n","    else: \n","        return np.concatenate((sketch[:,:-1],pen_up,end_stroke),axis=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def encode_dataset1(data,lengths,do_offset = True,N = Nclass):\n","    \"\"\"\n","    Encode pen states by creating a new array of sketch data.\n","    \n","    Parameters:\n","        data (iterable): object containing data for each sketch\n","        \n","    Returns:\n","        ndarray: object array containing encoded data for each sketch\n","    \"\"\"\n","    # new_data = np.empty(data.size,dtype=object)\n","    new_data = np.empty((data.shape[0], data.shape[1], stroke_dim+N), dtype=object)\n","\n","    for i, sketch in enumerate(data):\n","        new_data[i] = encode_sketch(sketch,lengths[i],N,do_offset)\n","\n","    return new_data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class SketchesDataset():\n","    def __init__(self, datasets, mode, transform=None):\n","        \"\"\"\n","        data_path: The path to the data\n","        mode: Either 'train' or 'test'\n","        \"\"\"\n","        self.transform = transform\n","        self.mode = mode\n","        self.data_set = []\n","        for i, dataset in enumerate(datasets):\n","            dataset = dataset[mode]\n","            dataset = normalize_data(dataset)\n","            for j, sketch in enumerate(dataset):\n","                sketch_class = np.full((len(sketch), 1), i)\n","                sketch = np.concatenate([sketch, sketch_class], 1)\n","                self.data_set.append(sketch)\n","\n","    def __len__(self):\n","        return len(self.data_set)\n","\n","    def __getitem__(self, idx):\n","        sketch = self.data_set[idx]\n","        if self.transform:\n","            sketch, length = self.transform(sketch)\n","        return sketch, length"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def normalize_data(data):\n","\n","    total_length = 0\n","    \n","\n","    for element in data:\n","        total_length += (len(element))\n","\n","\n","    coordinate_list = np.empty((total_length,2))\n","\n","    i = 0\n","\n","    for element in data:\n","        coordinate_list[i:i+len(element),:] = element[:,0:2]\n","        i+=len(element)\n","\n","    data_std = np.std(coordinate_list)\n","\n","    for i, element in enumerate(data):\n","        data[i] = data[i].astype(np.float32)\n","        data[i][:,0:2] = element[:,0:2].astype(np.float32)/data_std\n","    \n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def display_encoded_image(image):\n","    \"\"\"\n","    For some image tensor, draw the image using matplotlib.\n","\n","    Parameters:\n","        - image: some [n*5] tensor representing a sketch.\n","    Returns:\n","        - none\n","    \"\"\"\n","    #Xplot and Yplot are array of points that will be plotted\n","    Xplot = [0]\n","    Yplot = [0]\n","    #Keeps track of the current point that is being drawn\n","    xpos = 0\n","    ypos = 0\n","    #For loop to go through data and plot points\n","    i=0\n","    for i in range(len(image)):\n","        xpos += float(image[i,0])\n","        ypos += float(image[i,1])\n","        Xplot.append(-xpos)\n","        Yplot.append(-ypos)\n","        if image[i,3] == 0:\n","            plt.plot(Xplot, Yplot,color='black')\n","            Xplot.clear()\n","            Yplot.clear()\n","        # elif image[i, 4] == 1:\n","    ax = plt.gca()\n","    ax.set_aspect('equal', adjustable='box')\n","    ax.axis('off')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def kl_loss(sigma_hat, mu):\n","    # torch.sum is added to sum over all the dimensions of the vectors\n","    return (-0.5 / (latent_dim * batch_size)) * torch.sum(1 + sigma_hat - torch.square(mu) - torch.exp(sigma_hat))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def make_image(image):\n","    length = len(image)\n","    new_image = np.zeros((Nmax, 4))\n","    new_image[:len(image), :] = image[:len(image), :] # copy over values\n","    if conditional:\n","        # don't forget to stack input along dim = 1\n","        encoded_strokes = np.stack(encode_dataset1(np.array([new_image]),[length]), 1) \n","    else:\n","        encoded_strokes = np.stack(encode_dataset1(np.array([new_image]),[length],N=0), 1) \n","    batch = torch.from_numpy(encoded_strokes.astype(float))\n","    return batch, torch.tensor(length)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def make_batch(size=batch_size):\n","    \"\"\"\n","    Using the data created earlier in the code and a given batch size, randomly fetch\n","    that many images and return them + their lengths.\n","\n","    Parameters:\n","        - size: the size of the batch. Default is the variable batch_size declared\n","            at the start of the code.\n","\n","    Returns:\n","        - batch: a tensor of the batch of random images appended in the order they were fetched in.\n","        - lengths: the length of each image fetched, in the order they were fetched in.\n","    \"\"\"\n","\n","    batch_ids = np.random.choice(len(data), size)\n","    batch_images = [data[id] for id in batch_ids]\n","    lengths = [len(image) for image in batch_images]\n","    strokes = []\n","    for image in batch_images:\n","        new_image = np.zeros((Nmax, 3))\n","        new_image[:len(image), :] = image[:len(image), :] # copy over values\n","        strokes.append(new_image)\n","\n","    encoded_strokes = np.stack(encode_dataset1(np.array(strokes),lengths), 1) # don't forget to stack input along dim = 1\n","    batch = torch.from_numpy(encoded_strokes.astype(float))\n","    return batch, torch.tensor(lengths)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self):\n","        super(Encoder, self).__init__()\n","        if conditional:\n","            self.input_dim = stroke_dim + Nclass\n","        else:\n","            self.input_dim = stroke_dim\n","\n","        self.lstm = nn.LSTM(self.input_dim, enc_hidden_dim, bidirectional=True)\n","\n","        self.fc_mu = nn.Linear(2*enc_hidden_dim, latent_dim)\n","        self.fc_sigma = nn.Linear(2*enc_hidden_dim, latent_dim)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Runs a batch of images through the encoder and returns its latent vector.\n","        Does not normalize values on its own.\n","\n","        Parameters:\n","         - x: Tensor of shape [max_strokes, batch_size, num_features]\n","            where max_strokes is the highest number of points possible for an image in the batch.\n","            x should be normalized.\n","        - batch_size: int representing the current batch size.\n","\n","        Returns:\n","        - mu: Tensor of shape [batch_size, 2*hidden dim] representing the mean of the distribution of values\n","        - sigma: Tensor of shape [batch_size, 2*hidden dim] representing the log of the distribution of values\n","        \"\"\"\n","\n","        # Get the hidden states\n","        hidden, cell = torch.zeros(2, (batch_size), enc_hidden_dim,device=device), torch.zeros(2, (batch_size), enc_hidden_dim,device=device)\n","\n","        _, (hidden, cell) = self.lstm(x.float(), (hidden, cell))\n","        hidden_forward_dir, hidden_backward_dir = torch.split(hidden, 1, 0)\n","        hidden_concatenated = torch.cat([hidden_forward_dir.squeeze(0), hidden_backward_dir.squeeze(0)], 1)\n","\n","        mu = self.fc_mu(hidden_concatenated)\n","        sigma = self.fc_sigma(hidden_concatenated)\n","        return mu, sigma\n","\n","encoder = Encoder()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def gaussian_mixture_model(mixture_weights, mean_x, mean_y, std_x, std_y, corr_xy):\n","    \"\"\"\n","    Input:\n","        mixture_weights: Mixture weights (probability of a point being in distribution i)\n","        mean_x: The x-values of the means of each distribution\n","        mean_y: The y-values of the means of each distribution\n","        std_x: The standard deviations of the x-values\n","        std_y: The standard deviations of the y-values\n","        corr_xy: The correlation coefficients of the x and y values\n","        \n","    Return: \n","        The sampled x and y offsets\n","    \"\"\"\n","    # Choose which distribution to sample from\n","    mixture_weights = torch.reshape(mixture_weights,(batch_size,M)).contiguous() \n","     \n","    # Index for each batch\n","    i = torch.searchsorted(mixture_weights.cumsum(0), torch.rand(batch_size, 1)).squeeze()\n","    \n","    # Sample from bivariate normal distribution i\n","    rand_x, rand_y = torch.randn(batch_size), torch.randn(batch_size)\n","    \n","    mean_x = torch.take(mean_x, i)\n","    mean_y = torch.take(mean_y, i)\n","    std_x = torch.take(std_x, i)\n","    std_y = torch.take(std_y, i)\n","    corr_xy = torch.take(corr_xy, i)\n","    \n","    # Alternatively torch.distributions.multivariate_normal.MultivariateNormal?\n","    offset_x = mean_x + std_x * rand_x\n","    offset_y = mean_y + std_y * (corr_xy * offset_x + torch.sqrt(1 - corr_xy ** 2) * rand_y)\n","    return offset_x.unsqueeze(0), offset_y.unsqueeze(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def sample(mixture_weights, mean_x, mean_y, std_x, std_y, corr_xy, pen_state):\n","    offset_x, offset_y = gaussian_mixture_model(mixture_weights, mean_x, mean_y, std_x, std_y, corr_xy)\n","    \n","    pen_state = pen_state.squeeze()\n","    pen_state = torch.searchsorted(pen_state.cumsum(1), torch.rand(batch_size, 1)).squeeze()\n","    next_point = torch.cat((offset_x, offset_y, torch.zeros(3, batch_size)))\n","    next_point = torch.cat((offset_x, offset_y, torch.eye(3)[pen_state].transpose(0, 1)))\n","    return next_point.transpose(0, 1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def distribution(decoder_output):\n","    \"\"\"\n","    Input: \n","        decoder_output (6M + 3): Decoder LSTM output\n","    Return:\n","        mixture_weights (M): Mixture weights (probability of a point being in distribution i)\n","        mean_x (M): The x-values of the means of each distribution\n","        mean_y (M): The y-values of the means of each distribution\n","        std_x (M): The standard deviations of the x-values\n","        std_y (M): The standard deviations of the y-values\n","        corr_xy (M): The correlation coefficients of the x and y values\n","        q (3): The predicted pen state (pen_down, pen_up, <EOS>)\n","    \"\"\"\n","    sqrtT = sqrt(T)\n","    # Split the decoder output into \n","    # [pi, mean_x, mean_y, std_x, std_y, rho_xy] and [q1,q2,q3]\n","    parameters = torch.split(decoder_output, 6, 2)\n","\n","    # Chunk the parameters together, then stack them \n","    # so that each column defines a distribution\n","    mixture_parameters = torch.stack(parameters[:-1],1)\n","\n","    # Split mixture parameters into each parameter\n","    mixture_weights, mean_x, mean_y, std_x, std_y, corr_xy = torch.split(mixture_parameters, 1, 3)\n","\n","    # The 3 leftover parameters are for the pen state\n","    pen_state = parameters[-1]\n","\n","    mixture_weights = F.softmax(mixture_weights/T,dim=3)  # Each weight must be in [0, 1] and all must sum to 1\n","    std_x = torch.exp(std_x)*sqrtT  # Standard deviation must be positive\n","    std_y = torch.exp(std_y)*sqrtT  # Standard deviation must be positive\n","    corr_xy = F.tanh(corr_xy)  # Correlation coefficient must be in [-1, 1]\n","    pen_state = F.softmax(pen_state/T,dim=2)  # Each probability must be in [0, 1] and all must sum to 1\n","\n","    return mixture_weights, mean_x, mean_y, std_x, std_y, corr_xy, pen_state"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class HyperLSTMCell(Module):\n","    \"\"\"\n","    ## HyperLSTM Cell\n","\n","    For HyperLSTM the smaller network and the larger network both have the LSTM structure.\n","    This is defined in Appendix A.2.2 in the paper.\n","    \"\"\"\n","\n","    def __init__(self, input_size: int, hidden_size: int, hyper_size: int, n_z: int):\n","        \"\"\"\n","        `input_size` is the size of the input $x_t$,\n","        `hidden_size` is the size of the LSTM, and\n","        `hyper_size` is the size of the smaller LSTM that alters the weights of the larger outer LSTM.\n","        `n_z` is the size of the feature vectors used to alter the LSTM weights.\n","\n","        We use the output of the smaller LSTM to compute $z_h^{i,f,g,o}$, $z_x^{i,f,g,o}$ and\n","        $z_b^{i,f,g,o}$ using linear transformations.\n","        We calculate $d_h^{i,f,g,o}(z_h^{i,f,g,o})$, $d_x^{i,f,g,o}(z_x^{i,f,g,o})$, and\n","        $d_b^{i,f,g,o}(z_b^{i,f,g,o})$ from these, using linear transformations again.\n","        These are then used to scale the rows of weight and bias tensors of the main LSTM.\n","\n","        📝 Since the computation of $z$ and $d$ are two sequential linear transformations\n","        these can be combined into a single linear transformation.\n","        However we've implemented this separately so that it matches with the description\n","        in the paper.\n","        \"\"\"\n","        super().__init__()\n","\n","        # The input to the hyperLSTM is\n","        # $$\n","        # \\hat{x}_t = \\begin{pmatrix}\n","        # h_{t-1} \\\\\n","        # x_t\n","        # \\end{pmatrix}\n","        # $$\n","        # where $x_t$ is the input and $h_{t-1}$ is the output of the outer LSTM at previous step.\n","        # So the input size is `hidden_size + input_size`.\n","        #\n","        # The output of hyperLSTM is $\\hat{h}_t$ and $\\hat{c}_t$.\n","        self.hyper = LSTMCell(hidden_size + input_size, hyper_size, layer_norm=True)\n","\n","        # $$z_h^{i,f,g,o} = lin_{h}^{i,f,g,o}(\\hat{h}_t)$$\n","        # 🤔 In the paper it was specified as\n","        # $$z_h^{i,f,g,o} = lin_{h}^{i,f,g,o}(\\hat{h}_{\\textcolor{red}{t-1}})$$\n","        # I feel that it's a typo.\n","        self.z_h = nn.Linear(hyper_size, 4 * n_z)\n","        # $$z_x^{i,f,g,o} = lin_x^{i,f,g,o}(\\hat{h}_t)$$\n","        self.z_x = nn.Linear(hyper_size, 4 * n_z)\n","        # $$z_b^{i,f,g,o} = lin_b^{i,f,g,o}(\\hat{h}_t)$$\n","        self.z_b = nn.Linear(hyper_size, 4 * n_z, bias=False)\n","\n","        # $$d_h^{i,f,g,o}(z_h^{i,f,g,o}) = lin_{dh}^{i,f,g,o}(z_h^{i,f,g,o})$$\n","        d_h = [nn.Linear(n_z, hidden_size, bias=False) for _ in range(4)]\n","        self.d_h = nn.ModuleList(d_h)\n","        # $$d_x^{i,f,g,o}(z_x^{i,f,g,o}) = lin_{dx}^{i,f,g,o}(z_x^{i,f,g,o})$$\n","        d_x = [nn.Linear(n_z, hidden_size, bias=False) for _ in range(4)]\n","        self.d_x = nn.ModuleList(d_x)\n","        # $$d_b^{i,f,g,o}(z_b^{i,f,g,o}) = lin_{db}^{i,f,g,o}(z_b^{i,f,g,o})$$\n","        d_b = [nn.Linear(n_z, hidden_size) for _ in range(4)]\n","        self.d_b = nn.ModuleList(d_b)\n","\n","        # The weight matrices $W_h^{i,f,g,o}$\n","        self.w_h = nn.ParameterList([nn.Parameter(torch.zeros(hidden_size, hidden_size)) for _ in range(4)])\n","        # The weight matrices $W_x^{i,f,g,o}$\n","        self.w_x = nn.ParameterList([nn.Parameter(torch.zeros(hidden_size, input_size)) for _ in range(4)])\n","\n","        # Layer normalization\n","        self.layer_norm = nn.ModuleList([nn.LayerNorm(hidden_size) for _ in range(4)])\n","        self.layer_norm_c = nn.LayerNorm(hidden_size)\n","\n","    def forward(self, x: torch.Tensor,\n","                h: torch.Tensor, c: torch.Tensor,\n","                h_hat: torch.Tensor, c_hat: torch.Tensor):\n","        # $$\n","        # \\hat{x}_t = \\begin{pmatrix}\n","        # h_{t-1} \\\\\n","        # x_t\n","        # \\end{pmatrix}\n","        # $$\n","        x_hat = torch.cat((h, x), dim=-1)\n","        # $$\\hat{h}_t, \\hat{c}_t = lstm(\\hat{x}_t, \\hat{h}_{t-1}, \\hat{c}_{t-1})$$\n","        h_hat, c_hat = self.hyper(x_hat, h_hat, c_hat)\n","\n","        # $$z_h^{i,f,g,o} = lin_{h}^{i,f,g,o}(\\hat{h}_t)$$\n","        z_h = self.z_h(h_hat).chunk(4, dim=-1)\n","        # $$z_x^{i,f,g,o} = lin_x^{i,f,g,o}(\\hat{h}_t)$$\n","        z_x = self.z_x(h_hat).chunk(4, dim=-1)\n","        # $$z_b^{i,f,g,o} = lin_b^{i,f,g,o}(\\hat{h}_t)$$\n","        z_b = self.z_b(h_hat).chunk(4, dim=-1)\n","\n","        # We calculate $i$, $f$, $g$ and $o$ in a loop\n","        ifgo = []\n","        for i in range(4):\n","            # $$d_h^{i,f,g,o}(z_h^{i,f,g,o}) = lin_{dh}^{i,f,g,o}(z_h^{i,f,g,o})$$\n","            d_h = self.d_h[i](z_h[i])\n","            # $$d_x^{i,f,g,o}(z_x^{i,f,g,o}) = lin_{dx}^{i,f,g,o}(z_x^{i,f,g,o})$$\n","            d_x = self.d_x[i](z_x[i])\n","\n","            # \\begin{align}\n","            # {i,f,g,o} = LN(&\\textcolor{lightgreen}{d_h^{i,f,g,o}(z_h) \\odot (W_h^{i,f,g,o} h_{t-1})} \\\\\n","            #              + &\\textcolor{lightgreen}{d_x^{i,f,g,o}(z_x) \\odot (W_h^{i,f,g,o} x_t)} \\\\\n","            #              + &d_b^{i,f,g,o}(z_b))\n","            # \\end{align}\n","            y = d_h * torch.einsum('ij,bj->bi', self.w_h[i], h) + \\\n","                d_x * torch.einsum('ij,bj->bi', self.w_x[i], x) + \\\n","                self.d_b[i](z_b[i])\n","\n","            ifgo.append(self.layer_norm[i](y))\n","\n","        # $$i_t, f_t, g_t, o_t$$\n","        i, f, g, o = ifgo\n","\n","        # $$c_t = \\sigma(f_t) \\odot c_{t-1} + \\sigma(i_t) \\odot \\tanh(g_t) $$\n","        c_next = torch.sigmoid(f) * c + torch.sigmoid(i) * torch.tanh(g)\n","\n","        # $$h_t = \\sigma(o_t) \\odot \\tanh(LN(c_t))$$\n","        h_next = torch.sigmoid(o) * torch.tanh(self.layer_norm_c(c_next))\n","\n","        return h_next, c_next, h_hat, c_hat\n","\n","\n","class HyperLSTM(Module):\n","    \"\"\"\n","    # HyperLSTM module\n","    \"\"\"\n","\n","    def __init__(self, input_size: int, hidden_size: int, hyper_size: int, n_z: int, n_layers: int):\n","        \"\"\"\n","        Create a network of `n_layers` of HyperLSTM.\n","        \"\"\"\n","\n","        super().__init__()\n","\n","        # Store sizes to initialize state\n","        self.n_layers = n_layers\n","        self.hidden_size = hidden_size\n","        self.hyper_size = hyper_size\n","\n","        # Create cells for each layer. Note that only the first layer gets the input directly.\n","        # Rest of the layers get the input from the layer below\n","        self.cells = nn.ModuleList([HyperLSTMCell(input_size, hidden_size, hyper_size, n_z)] +\n","                                   [HyperLSTMCell(hidden_size, hidden_size, hyper_size, n_z) for _ in\n","                                    range(n_layers - 1)])\n","\n","    def forward(self, x: torch.Tensor,\n","                state: Optional[Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]] = None):\n","        \"\"\"\n","        * `x` has shape `[n_steps, batch_size, input_size]` and\n","        * `state` is a tuple of $h, c, \\hat{h}, \\hat{c}$.\n","         $h, c$ have shape `[batch_size, hidden_size]` and\n","         $\\hat{h}, \\hat{c}$ have shape `[batch_size, hyper_size]`.\n","        \"\"\"\n","        n_steps, batch_size = x.shape[:2]\n","\n","        # Initialize the state with zeros if `None`\n","        if state is None:\n","            h = [x.new_zeros(batch_size, self.hidden_size) for _ in range(self.n_layers)]\n","            c = [x.new_zeros(batch_size, self.hidden_size) for _ in range(self.n_layers)]\n","            h_hat = [x.new_zeros(batch_size, self.hyper_size) for _ in range(self.n_layers)]\n","            c_hat = [x.new_zeros(batch_size, self.hyper_size) for _ in range(self.n_layers)]\n","        #\n","        else:\n","            (h, c, h_hat, c_hat) = state\n","            # Reverse stack the tensors to get the states of each layer\n","            #\n","            # 📝 You can just work with the tensor itself but this is easier to debug\n","            h, c = list(torch.unbind(h)), list(torch.unbind(c))\n","            h_hat, c_hat = list(torch.unbind(h_hat)), list(torch.unbind(c_hat))\n","\n","        # Collect the outputs of the final layer at each step\n","        out = []\n","        for t in range(n_steps):\n","            # Input to the first layer is the input itself\n","            inp = x[t]\n","            # Loop through the layers\n","            for layer in range(self.n_layers):\n","                # Get the state of the layer\n","                h[layer], c[layer], h_hat[layer], c_hat[layer] = \\\n","                    self.cells[layer](inp, h[layer], c[layer], h_hat[layer], c_hat[layer])\n","                # Input to the next layer is the state of this layer\n","                inp = h[layer]\n","            # Collect the output $h$ of the final layer\n","            out.append(h[-1])\n","\n","        # Stack the outputs and states\n","        out = torch.stack(out)\n","        h = torch.stack(h)\n","        c = torch.stack(c)\n","        h_hat = torch.stack(h_hat)\n","        c_hat = torch.stack(c_hat)\n","\n","        #\n","        return out, (h, c, h_hat, c_hat)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def bivariate_normal_pdf(dx, dy, mu_x, mu_y, std_x, std_y, corr_xy):\n","    \"\"\"\n","    Return N(dx, dy | mu_x, mu_y, std_x, std_y, corr_xy)\n","    \"\"\"\n","    z_x = (dx - mu_x) / std_x\n","    z_y = (dy - mu_y) / std_y\n","    exponent = -(z_x ** 2 - 2 * corr_xy * z_x * z_y + z_y ** 2) / (2 * (1 - corr_xy ** 2))\n","    norm = 2 * np.pi * std_x * std_y * torch.sqrt(1-corr_xy ** 2)\n","    return torch.exp(exponent) / norm\n","\n","\n","def offset_reconstruction_loss(dx, dy, pi, mu_x, mu_y, std_x, std_y, corr_xy, mask):\n","    \"\"\"\n","    pi: The mixture probabilities\n","    mask: 1 if the point is not after the final stroke, 0 otherwise\n","\n","    Returns the reconstruction loss for the strokes, L_s\n","    \"\"\"\n","    pdf = bivariate_normal_pdf(dx, dy, mu_x, mu_y, std_x, std_y, corr_xy)\n","    \n","    return -torch.sum(mask * torch.log(1e-5 + torch.sum(pi*pdf,axis=0))) / (batch_size*Nmax) \n","\n","\n","def pen_reconstruction_loss(input_pen_state, output):\n","    \"\"\"\n","    Parameters:\n","\n","        N_max (int) - Maximum sketch sequence length\n","        \n","        input_pen_state (batch_size,3) - Pen state data for a stroke\n","\n","        output (batch_size, 3)- Generated pen state logit values.\n","\n","    Returns:\n","        Reconstruction loss for pen state.\n","    \"\"\"    \n","\n","    return -torch.sum(input_pen_state*torch.log(1e-5+output)) / (batch_size*Nmax)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#DECODER..\n","\n","output_dim = 6*M + 3\n","\n","class Decoder(nn.Module):\n","    def __init__(self):\n","        super(Decoder, self).__init__()\n","\n","       # input dimension is z | (x,y,p1,p2,p3) | (c1, c2, ... c_Nclass)\n","        if conditional:\n","            self.input_dim = latent_dim + stroke_dim + Nclass\n","        else:\n","            self.input_dim = latent_dim + stroke_dim\n","\n","        # generates initial hidden and cell states from latent vector\n","        self.fc_in = nn.Linear(latent_dim,2*dec_hidden_dim + 2*dec_hyper_dim)\n","\n","        # Fully connected layer for reducing dimensionality of hidden state before\n","        # being used for distribution parameters.\n","        self.fc_proj = nn.Linear(dec_hidden_dim,output_dim)\n","\n","        # Input has dimension latent_dim + 5 for latent vector and initial stroke\n","        self.lstm = HyperLSTM(self.input_dim,dec_hidden_dim,dec_hyper_dim,feature_dim,1)\n","\n","        self.hidden = torch.zeros(batch_size,dec_hidden_dim,device=device)\n","        self.cell = torch.zeros(batch_size,dec_hidden_dim,device=device)\n","        \n","        self.hidden_h = torch.zeros(batch_size,dec_hyper_dim,device=device)\n","        self.cell_h = torch.zeros(batch_size,dec_hyper_dim,device=device)\n","    \n","    def dec_forward1(self, z, strokes, classifier = None):\n","        params = [torch.zeros(Nmax,M,batch_size).to(device) for  _ in range(6)] + [torch.zeros(Nmax,batch_size,3).to(device)]\n","        # used when loop goes beyond input sketch length\n","        empty_stroke = torch.tensor([0,0,0,0,1]).to(torch.float32).to(device)\n","        # For each timestep, pass the batch of strokes through LSTM and compute\n","        # the output.  Output of the previous timestep is used as input.\n","        for i in range(1,Nmax+1):\n","            if classifier == None:\n","                input = strokes[i-1]\n","            else:\n","                input = torch.cat((strokes[i-1],classifier),dim = 1)\n","            \n","            x = torch.cat((input,z),dim = 1).view(1,batch_size,self.input_dim)\n","            \n","            (h,c,h_hat,c_hat) = self.lstm.cells[0](x.float().squeeze(), \n","                                    self.hidden.squeeze(), \n","                                    self.cell.squeeze(), \n","                                    self.hidden_h.squeeze(), \n","                                    self.cell_h.squeeze())\n","            self.hidden = h.unsqueeze(0)\n","            self.cell = c.unsqueeze(0)\n","            self.hidden_h = h_hat.unsqueeze(0)\n","            self.cell_h = c_hat.unsqueeze(0)    \n","            \n","            output = self.hidden[-1].unsqueeze(0)\n","            with torch.device(device):\n","                #params will be used for computing loss\n","                param_list = distribution(self.fc_proj(output))\n","                strokes[i,:,:] = sample(*param_list).to(device)\n","                for j in range(7):\n","                    params[j][i-1,:,:] = param_list[j].squeeze()\n","            \n","            stroke_mask = (strokes[i,:,4] == 1).to(device) # boolean mask set to true when p3 == 1\n","            \n","            strokes[i,stroke_mask.squeeze(),:] = empty_stroke\n","            \n","        return params\n","    \n","    def dec_forward2(self, z, strokes):\n","        self.hidden, self.cell, self.hidden_h, self.cell_h = torch.split(\n","            F.tanh(self.fc_in(z).unsqueeze(0)),\n","            [dec_hidden_dim, dec_hidden_dim, dec_hyper_dim, dec_hyper_dim],\n","            dim = 2)\n","        out, (self.hidden, self.cell,self.hidden_h,self.cell_h) = self.lstm(strokes.float(),(self.hidden.contiguous(), \n","                                           self.cell.contiguous(),\n","                                           self.hidden_h.contiguous(),\n","                                           self.cell_h.contiguous()))\n","        params = distribution(self.fc_proj(out))\n","        return params\n","    \n","    def dec_forward3(self,z,strokes,classifier):\n","        params = self.dec_forward2(z,strokes)\n","        N_s = strokes.shape[0]\n","        new_strokes = torch.zeros(N_s, batch_size, stroke_dim,device=device)\n","        with torch.device(device):\n","            for i in range(N_s):\n","                new_strokes[i,:,:] = sample(*[j[i] for j in params])\n","        \n","        generated_strokes = [new_strokes[-1,:,:]]\n","        i = 1\n","        while True:\n","            if classifier == None:\n","                input = generated_strokes[i-1]\n","            else:\n","                input = torch.cat((generated_strokes[i-1],classifier),dim = 1)\n","            x = torch.cat((input,z),dim = 1).view(1,batch_size,self.input_dim)\n","            \n","            (h,c,h_hat,c_hat) = self.lstm.cells[0](x.float().squeeze(), \n","                                    self.hidden.squeeze(), \n","                                    self.cell.squeeze(), \n","                                    self.hidden_h.squeeze(), \n","                                    self.cell_h.squeeze())\n","            self.hidden = h.unsqueeze(0)\n","            self.cell = c.unsqueeze(0)\n","            self.hidden_h = h_hat.unsqueeze(0)\n","            self.cell_h = c_hat.unsqueeze(0)    \n","            \n","            output = self.hidden[-1].unsqueeze(0)\n","            with torch.device(device):\n","                #params will be used for computing loss\n","                params = distribution(self.fc_proj(output))\n","                generated_strokes.append(sample(*params).to(device))\n","            \n","            if generated_strokes[i][0,4].item() == 1 or i >= Nmax - N_s:\n","                break\n","            i = i + 1\n","            \n","        return torch.stack(generated_strokes[1:],dim = 0)\n","    \n","    \n","    def forward(self, z, strokes, mode = 'train',classifier = None):\n","        \"\"\"\n","        Parameters:\n","            z - Tensor of size  (batch_size, latent_dim), with latent vector samples.\n","\n","            stroke - Tensor of size (batch_size, stroke dim); previous stroke\n","\n","        Returns:\n","            Tensor of size (N_max, batch_size, stroke dim), as the next stroke\n","        \"\"\"\n","        \n","        if mode == 'generate':\n","            params = self.dec_forward1(z,strokes,classifier)\n","            return params\n","        \n","        elif mode == 'train':\n","            params = self.dec_forward2(z,strokes)\n","            return params\n","            \n","        elif mode == 'finish-sketch':\n","            params\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class VAE(nn.Module):\n","    def __init__(self):\n","        super(VAE, self).__init__()\n","        self.encoder = Encoder()\n","        self.decoder = Decoder()\n","        self.generate = False\n","        \n","    \n","    def run_decoder_generate(self,batch,lengths,z,classifier,compute_loss = True):\n","        '''\n","        Generates sketches conditioned only on latent vector and a classification vector\n","        \n","        Parameters:\n","            classifer (1,Nclass) - One-hot encoded (usually) vector to indicate sketch type.\n","            \n","            compute_loss - Set this to False if loss is not needed.\n","        '''\n","       \n","        # Obtain initial hidden and cell states by splitting result of fc_in along column axis\n","        self.decoder.hidden, self.decoder.cell, self.decoder.hidden_h, self.decoder.cell_h = torch.split(\n","                F.tanh(self.decoder.fc_in(z).unsqueeze(0)),\n","                [dec_hidden_dim, dec_hidden_dim, dec_hyper_dim, dec_hyper_dim],\n","                dim = 2)\n","        \n","        # Data for all strokes in output sequence\n","        strokes = torch.zeros(Nmax + 1, batch_size, stroke_dim,device=device)\n","        strokes[0,:] = torch.tensor([0,0,1,0,0]).to(device)\n","        \n","        \n","        params = self.decoder(z,strokes,'generate',classifier=classifier)\n","                   \n","        #mask, dx, dy, p = make_target(batch, lengths)\n","        #offset_params = [params[i].squeeze().transpose(0, 1) for i in range(6)]\n","        #l_p = pen_reconstruction_loss(p, params[6]) \n","        #l_s = offset_reconstruction_loss(dx, dy, *offset_params, mask[:-1])\n","                \n","        return strokes[1:], params\n","\n","    def run_decoder_train(self,batch,lengths,z):\n","\n","        empty = torch.zeros(stroke_dim + Nclass)\n","        empty[2] = 1\n","        start_stroke = torch.stack([empty] * batch_size).unsqueeze(0).to(device)\n","        strokes = torch.cat([start_stroke, batch[:-1]], 0)\n","        zs = torch.stack([z] * (Nmax))\n","        #IMPORTANT: Must always ensure that this is concatenated in the same order as the\n","        #generation mode in the decoder. \n","        strokes = torch.cat([strokes,zs], 2)\n","\n","        params = self.decoder(z, strokes,'train')\n","        \n","        output = torch.zeros(Nmax, batch_size, 5,device=device)\n","        with torch.device(device):\n","            empty_stroke = torch.tensor([0,0,0,0,1]).to(torch.float32).to(device)\n","            for i in range(Nmax):\n","                output[i] = sample(*[j[i] for j in params])\n","                stroke_mask = (i > lengths).squeeze()\n","                output[i,stroke_mask,:] = empty_stroke\n","                           \n","\n","        return output, params\n","    \n","    def complete_sketch(self,input,label = None):\n","        mean, logvar = self.encoder(input)\n","        z = mean + torch.exp(logvar/2)*torch.randn(batch_size, latent_dim, device = device)\n","        empty = torch.zeros(stroke_dim + Nclass) if conditional else torch.zeros(stroke_dim)\n","        empty[2] = 1\n","        start_stroke = torch.stack([empty] * batch_size).unsqueeze(0).to(device)\n","        strokes = torch.cat([start_stroke, input[:-1]], 0)\n","        zs = torch.stack([z] * (input.shape[0]))\n","        #IMPORTANT: Must always ensure that this is concatenated in the same order as the\n","        #generation mode in the decoder. \n","        strokes = torch.cat([strokes,zs], 2)\n","        \n","        if conditional:\n","            classifier = input[0,:,5:].clone() if label == None else label\n","        else:\n","            classifier = None\n","        \n","        output = self.decoder.dec_forward3(z,strokes,classifier = classifier)\n","        \n","        return torch.cat((input[:,:,:5],output),dim = 0)\n","    \n","    def forward(self, batch, lengths, anneal_loss=True, step=0): \n","        batch = batch.to(device)\n","        lengths = lengths.to(device) \n","        \n","        mean, logvar = self.encoder(batch)\n","        \n","        # sample latent vector from encoder output\n","        random_sample = torch.randn(batch_size, latent_dim, device = device)\n","        std = torch.exp(logvar/2) # logvar / 2 should be a float\n","        z = mean + std*random_sample\n","        \n","        if conditional:\n","            classifier = batch[0,:,5:].clone()\n","        else:\n","            classifier = None\n","        \n","        if self.generate:\n","            output, params = self.run_decoder_generate(batch,lengths,z,classifier)\n","        else:\n","            output, params = self.run_decoder_train(batch,lengths,z)\n","            \n","        mask, dx, dy, p = make_target(batch, lengths)\n","        \n","        #compute loss\n","        offset_params = [params[i].squeeze().transpose(0, 1) for i in range(6)]\n","        l_p = pen_reconstruction_loss(p, params[6]) \n","        l_s = offset_reconstruction_loss(dx, dy, *offset_params, mask[:-1])\n","\n","        l_r = l_p + l_s\n","        l_kl = kl_loss(mean, logvar)\n","        \n","        if anneal_loss:\n","            # Calculate n_step\n","            n_step = 1 - (1 - n_min) * R**step\n","\n","            # Calculate the total weighted loss\n","            loss = l_r + w_kl * n_step * max(l_kl, KL_min)\n","        else:\n","            loss = l_r + w_kl * l_kl\n","    \n","        return output, loss, l_kl, l_s, l_p"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def make_target(batch, lengths):\n","    with torch.device(device):\n","        mask = torch.zeros((Nmax + 1, batch.size()[1]))\n","        for index, num_strokes in enumerate(lengths):\n","            mask[:num_strokes, index] = 1\n","\n","        dx = batch[:, :, 0]\n","        dy = batch[:, :, 1]\n","        # copy + append together pen state values\n","        p = torch.stack([batch.data[:, :, 2], batch.data[:, :, 3], batch.data[:, :, 4]], 2)\n","\n","        return mask, dx, dy, p"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def save_model(dir,filename = None, max_count = 2**31 - 1):\n","    try:\n","        os.makedirs(dir)\n","    except:\n","        pass\n","    \n","    if filename == None:\n","        filename = f\"{str(datetime.now())}.pt\"\n","        \n","    files = os.listdir(dir)\n","    \n","    while len(files) >= max_count:\n","        os.remove(dir + files.pop(0))\n","        \n","    torch.save({\n","            \"model\": model.state_dict(),\n","            \"opt\": optimizer.state_dict()\n","        }, dir + filename)\n","    \n","def load_model(dir,file_index = None):\n","    if not file_index == None:\n","        files = os.listdir(dir)\n","        dir = dir + files[file_index]\n","        \n","    loaded_state = torch.load(dir, map_location=device)\n","    model.load_state_dict(loaded_state['model'])\n","    optimizer.load_state_dict(loaded_state['opt'])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def train(model,optimizer,dataloader):\n","    print(\"Training loop running...\\n\")\n","    if device == 'cuda': torch.cuda.reset_peak_memory_stats()\n","    torch.cuda.empty_cache()\n","    lmbda = lambda epoch: 0.92\n","    scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)\n","    \n","    for epoch in range(n_epochs):\n","        for step, (batch, lengths) in enumerate(tqdm(dataloader)):\n","\n","            batch = batch.squeeze(2).transpose(0, 1)\n","                \n","            optimizer.zero_grad()\n","\n","            output, loss, l_kl, l_s, l_p = model(batch, lengths, anneal_loss=True, step=epoch * len(dataloader) + step)\n","\n","            loss.backward()\n","\n","            grad_threshold = 1.0 # tunable parameter, prevents exploding gradient\n","            nn.utils.clip_grad_norm_(model.encoder.parameters(), grad_threshold)\n","            nn.utils.clip_grad_norm_(model.decoder.parameters(), grad_threshold)\n","\n","            # update encoder and decoder parameters using adam algorithm\n","            optimizer.step()\n","\n","            for i in range(batch_size):\n","                display_encoded_image(output[:, i, :])\n","                display_encoded_image(batch[:, i, :])\n","            return\n","\n","            if step % 100 == 0:\n","                print(f\"Epoch: {epoch + 1}, Step: {step + 1}, Loss: {loss.item()}\")\n","                print(f\"l_kl: {l_kl.item():.4f} l_s: {l_s.item():.4f} l_p: {l_p.item():.4f}\") \n","                print(\"---------------------------------------------------------\\n\")\n","                # draw image\n","                display_encoded_image(output[:, 0, :])\n","                display_encoded_image(batch[:, 0, :])\n","                \n","\n","            if step % len(train_dataloader)/(10*batch_size) == 0:\n","                  save_model(\"model/temp/\",max_count=3)\n","                  torch.cuda.empty_cache()\n","\n","        scheduler.step()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def run_tests(model,dataloader,iterations = 2**31 - 1, display = False):\n","    if device == 'cuda': torch.cuda.reset_peak_memory_stats()\n","    torch.cuda.empty_cache()\n","\n","    \n","\n","    for step, (batch, lengths) in enumerate(tqdm(dataloader)):\n","        if (step+1 > iterations):\n","            return \n","        \n","        with torch.no_grad():\n","            batch = batch.squeeze(2).transpose(0, 1)\n","            S1 = torch.stack((batch[:,0,:],torch.zeros((Nmax,stroke_dim+Nclass))),dim=1)\n","            S2 = torch.stack((batch[:,1,:],torch.zeros((Nmax,stroke_dim+Nclass))),dim=1)\n","            out1, _, _, _, _ = model(S1, lengths, anneal_loss=False)\n","            out2, _, _, _, _ = model(S2, lengths, anneal_loss=False)\n","        \n","        #draw image\n","        if display:\n","            display_encoded_image(out1[:,0,:])\n","            display_encoded_image(batch[:, 0, :])\n","            display_encoded_image(out2[:,0,:])\n","            display_encoded_image(batch[:, 1, :])\n","    return "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["conditional = True\n","\n","train_dataset = SketchesDataset(\n","        datasets=datasets,\n","        mode='train',\n","        transform=make_image\n","    )\n","test_dataset = SketchesDataset(\n","        datasets=datasets,\n","        mode='test',\n","        transform=make_image\n","    )\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,drop_last=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True,drop_last=True)\n","\n","model = VAE().to(device)\n","optimizer = Adam(model.parameters(), lr = lr) \n","\n","if conditional:\n","    load_model(\"model/final/local/fruit.pt\")\n","else:\n","    load_model(\"model/final/local/fruit-unconditional.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.generate = False\n","train(model,optimizer,train_dataloader)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["save_model(\"model/final/local/\",filename=\"fruit-unconditional.pt\")"]},{"cell_type":"code","execution_count":68,"metadata":{"trusted":true},"outputs":[],"source":["model.generate = True\n","\n","def save_sketch(path,my_array):\n","    my_array = np.array(my_array.cpu())\n","    #Xplot and Yplot are array of points that will be plotted\n","    Xplot = []\n","    Yplot = []\n","    #Keeps track of the current point that is being drawn\n","    xpos = 0\n","    ypos = 0\n","    #While loop to go through data and plot points\n","    i=0\n","    while i < len(my_array):    \n","        xpos += my_array[i,0]\n","        ypos += my_array[i,1]\n","        Xplot.append(xpos)\n","        Yplot.append(-ypos)\n","        if my_array[i,2] == 1:\n","            plt.plot(Xplot, Yplot,color='black', linewidth=3)\n","            Xplot = []\n","            Yplot = []\n","        i+=1\n","    ax = plt.gca()\n","    ax.set_aspect('equal', adjustable='box')\n","    ax.axis('off')\n","    plt.savefig(path,dpi = 100)\n","    plt.clf()\n","\n","def save_tests(model,dataloader,iterations = 2**31 - 1, display = False):\n","    for step, (batch, lengths) in enumerate(tqdm(dataloader)):\n","        if (step+1 > iterations):\n","            return \n","        \n","        with torch.no_grad():\n","            batch = batch.squeeze(2).transpose(0, 1)\n","            S1 = torch.stack((batch[:,0,:],torch.zeros((Nmax,stroke_dim+Nclass))),dim=1)\n","            S2 = torch.stack((batch[:,1,:],torch.zeros((Nmax,stroke_dim+Nclass))),dim=1)\n","            out1, _, _, _, _ = model(S1, lengths, anneal_loss=False)\n","            out2, _, _, _, _ = model(S2, lengths, anneal_loss=False)\n","        \n","            save_sketch(f\"test_results/output/out{2*step}.png\",out1[:,0,:])\n","            save_sketch(f\"test_results/output/out{2*step+1}.png\",out2[:,0,:])\n","            save_sketch(f\"test_results/input/in{2*step}.png\",batch[:, 0, :])     \n","            save_sketch(f\"test_results/input/in{2*step+1}.png\",batch[:, 1, :])\n","    return \n","\n","def clear_test_dirs():\n","    inputs = os.listdir(\"test_results/input/\")\n","    outputs = os.listdir(\"test_results/output/\")\n","\n","    for input, output in zip(inputs, outputs):\n","        os.remove(\"test_results/input/\" + input)\n","        os.remove(\"test_results/output/\" + output)\n","    \n","T = 0.2\n","clear_test_dirs()\n","#save_tests(model,test_dataloader,display=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def latent_lerp(model, S1, S2, nstep):\n","    mean, logvar = model.encoder(S1)\n","    z1 = mean + torch.exp(logvar/2)*torch.randn(batch_size, latent_dim, device = device)\n","    mean, logvar = model.encoder(S2)\n","    z2 = mean + torch.exp(logvar/2)*torch.randn(batch_size, latent_dim, device = device)\n","    \n","    c1 = S1[0,:,5:].clone()\n","    c2 = S2[0,:,5:].clone()\n","    \n","    step_size = 1/(nstep - 1)\n","    \n","    z_interp = [torch.lerp(z1, z2, step_size*i) for i in range(nstep)]\n","    c_interp = [torch.lerp(c1, c2, step_size*i) for i in range(nstep)]\n","    \n","    return [model.run_decoder_generate(None, None, z, c, compute_loss = False)[0] for z, c in zip(z_interp, c_interp)]\n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# INTERPOLATION EXAMPLE\n","def get_sketch(dataloader):\n","    return next(iter(dataloader))[0].squeeze(2).transpose(0, 1).to(device)\n","\n","S1 = get_sketch(test_dataloader)\n","S2 = get_sketch(test_dataloader)\n","display_encoded_image(S1[:, 0, :])\n","display_encoded_image(S2[:, 0, :])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#SKETCH COMPLETION TESTS\n","\n","length = 6\n","end = torch.Tensor([0,0,0,0,1,0,0,0,0]).view((1,stroke_dim+Nclass))\n","N = 3\n","T = 0.1\n","curve = torch.Tensor([[-0.2871,  0.9499,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,0.0000],\n","                      [ 0.0000,  0.8311,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,0.0000],\n","                      [ 1.1022,  1.3577,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000, 0.0000],\n","                      [ 0.4270,  0.2801,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,0.0000],\n","                      [ 0.5866,  0.2271,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,0.0000],\n","                      [ 1.0083,  0.0365,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,0.0000],\n","                      [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,0.0000]])\n","\n","label = torch.cat([F.one_hot((torch.Tensor([N]).long()),Nclass).view(1,Nclass)]*batch_size,dim=0)\n","\n","\n","display_encoded_image(curve)\n","\n","ntests = 5\n","\n","for i in range(0, ntests):\n","    completion = model.complete_sketch(torch.stack([curve]*batch_size,dim=1)[:,:,:],label)\n","    display_encoded_image(completion[:, 0, :])\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["nstep = 7\n","T =0.2\n","\n","interp = latent_lerp(model,S1,S2,nstep)\n","\n","for i in range(nstep):\n","    display_encoded_image(interp[i][:, 0, :])\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":22085,"sourceId":28618,"sourceType":"datasetVersion"},{"datasetId":4481448,"sourceId":7681239,"sourceType":"datasetVersion"}],"dockerImageVersionId":30648,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":4}
